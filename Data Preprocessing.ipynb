{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the Glove file is not in the directory. Otherwise skip the command line section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-08-04 14:57:36--  http://nlp.stanford.edu/data/glove.twitter.27B.zip\n",
      "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://nlp.stanford.edu/data/glove.twitter.27B.zip [following]\n",
      "--2019-08-04 14:57:36--  https://nlp.stanford.edu/data/glove.twitter.27B.zip\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: http://downloads.cs.stanford.edu/nlp/data/glove.twitter.27B.zip [following]\n",
      "--2019-08-04 14:57:36--  http://downloads.cs.stanford.edu/nlp/data/glove.twitter.27B.zip\n",
      "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
      "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1520408563 (1.4G) [application/zip]\n",
      "Saving to: â€˜glove.twitter.27B.zipâ€™\n",
      "\n",
      "glove.twitter.27B.z 100%[===================>]   1.42G  4.76MB/s    in 4m 19s  \n",
      "\n",
      "2019-08-04 15:01:55 (5.60 MB/s) - â€˜glove.twitter.27B.zipâ€™ saved [1520408563/1520408563]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget \"http://nlp.stanford.edu/data/glove.twitter.27B.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  glove.twitter.27B.zip\n",
      "  inflating: glove.twitter.27B.25d.txt  \n",
      "  inflating: glove.twitter.27B.50d.txt  \n",
      "  inflating: glove.twitter.27B.100d.txt  \n",
      "  inflating: glove.twitter.27B.200d.txt  \n"
     ]
    }
   ],
   "source": [
    "!unzip -j glove.twitter.27B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import emoji\n",
    "from emoji import UNICODE_EMOJI\n",
    "import pandas as pd\n",
    "import glob\n",
    "import json\n",
    "import numpy as np\n",
    "import nltk\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers import MaxPool1D\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import SpatialDropout1D\n",
    "from keras.layers import Dropout\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers.embeddings import Embedding\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conventional Cleaning\n",
    "The function `clean_txt(tweet)` cleans the text. It:\n",
    "Converts to ASCII, convert to lowercase, separate punctuation, remove tokens with numbers, remove usernames, removes links, and stores the tweet as a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from unicodedata import normalize\n",
    "def clean_txt(tweet):\n",
    "    re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    re_print = re.compile('[^%s]' % re.escape(string.printable))\n",
    "    tweet = normalize('NFD', tweet).encode('ascii', 'ignore')\n",
    "    tweet = tweet.decode('UTF-8')\n",
    "    tweet = tweet.split()\n",
    "    tweet = [str(word) for word in tweet]\n",
    "    tweet = [word for word in tweet if word[0] != '@']\n",
    "    tweet = [word.lower() for word in tweet]\n",
    "    tweet = [word for word in tweet if word[0:4] != 'http']\n",
    "    tweet = [re_punc.sub('', w) for w in tweet]\n",
    "    tweet = [re_print.sub('', w) for w in tweet]\n",
    "    tweet = [word for word in tweet if word.isalpha()]\n",
    "    tweet = (' '.join(tweet))\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove instances of 'RT' from beginning of tweet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rm_rt(tweet):\n",
    "    rt = re.compile(r'^(RT|rt)')\n",
    "    if rt.search(tweet):\n",
    "        return ''\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the data: this includes all tweets that have been classified (both manually and by an automated labeling service):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>In my personal opinion, if you plan on committ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>@samtripoli they really going to allow the has...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>That feel when you can't wait for work to be o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>So, Let it be said that Amonute Matoaka Powhat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>https://t.co/t9BIoDCnm4  Also, Your Turn to Di...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  label                                               text\n",
       "0           0    0.0  In my personal opinion, if you plan on committ...\n",
       "1           1    0.0  @samtripoli they really going to allow the has...\n",
       "2           2    0.0  That feel when you can't wait for work to be o...\n",
       "3           3    0.0  So, Let it be said that Amonute Matoaka Powhat...\n",
       "4           4    3.0  https://t.co/t9BIoDCnm4  Also, Your Turn to Di..."
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = pd.read_csv('depression_final_data.csv')\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean up by dropping any NaN vals, label/clean up columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11538, 3)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = tweets.dropna()\n",
    "tweets.text = tweets.text.apply(clean_txt).apply(rm_rt)\n",
    "tweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.to_csv('cleaned_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMOTE Oversampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Smote is a technique to generate more samples of the minority class, in this case depressive tweets. While theoretically useful, it ultimately didn't yield too much of a result improvment compared to simply specifying class weights, so **is therefore not used** but is included nonetheless to demonstrate the work done. \n",
    "\n",
    "Inspired by: Mr. Theo Viel (Original article published under [Apache 2.0](https://www.apache.org/licenses/LICENSE-2.0) ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>in my personal opinion if you plan on committi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>they really going to allow the hashtag clinton...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>that feel when you cant wait for work to be ov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>so let it be said that amonute matoaka powhata...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.0</td>\n",
       "      <td>also your turn to die is now officially finish...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text\n",
       "0    0.0  in my personal opinion if you plan on committi...\n",
       "1    0.0  they really going to allow the hashtag clinton...\n",
       "2    0.0  that feel when you cant wait for work to be ov...\n",
       "3    0.0  so let it be said that amonute matoaka powhata...\n",
       "4    3.0  also your turn to die is now officially finish..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classified = pd.read_csv('cleaned_data.csv')[['label','text']]\n",
    "classified.text = classified.text.astype(str)\n",
    "classified.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean up by dropping any NaN vals, label/clean up columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "classified = classified.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use GloVe pretrained embeddings to seed embedding layer weights. Load GloVe weights trained on twitter dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1193514 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_idx = dict()\n",
    "glove_file = open('glove.twitter.27B.100d.txt')\n",
    "for line in glove_file:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_idx[word] = coefs\n",
    "glove_file.close()\n",
    "print('Loaded %s word vectors.' % len(embeddings_idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity, clump \"depressed\" and \"suicidal\" labels into one category (can refine this in future models/iteration; for first iteration, use binary classification):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = np.asarray(classified.text)\n",
    "labels_binary = classified.label.replace(to_replace=3, value=1)\n",
    "label = np.asarray(classified.label)\n",
    "\n",
    "np.unique(labels_binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize tweet texts. Use a max sentence length of 50 (with char limit of 280 and cleaning, this seems reasonable - by observation, most tweets still require padding), and pad all tweets out to this length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(text)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "# integer encode the documents\n",
    "encoded = tokenizer.texts_to_sequences(text)\n",
    "# pad documents to a max length of 4 words\n",
    "max_length = 50\n",
    "padded_tweets = pad_sequences(encoded, maxlen=max_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  9,   8, 800, ...,   0,   0,   0],\n",
       "       [ 34,  88,  90, ...,   0,   0,   0],\n",
       "       [ 14, 100,  53, ...,   0,   0,   0],\n",
       "       ...,\n",
       "       [ 31,  18, 777, ...,   0,   0,   0],\n",
       "       [686, 195,  12, ...,   0,   0,   0],\n",
       "       [378,  10, 986, ...,   0,   0,   0]], dtype=int32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split into train/val set of 90%/10%;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_train, text_test, label_train, label_test = train_test_split(padded_tweets, labels_binary, test_size = 0.1, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize embedding matrix to seed embedding weights from the loaded GloVe embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((vocab_size, 100))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_idx.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Place word indicies in a dictionary\n",
    "index_word = {0: ''}\n",
    "for word in tokenizer.word_index.keys():\n",
    "    index_word[tokenizer.word_index[word]] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# initialise Nearest Neighbor from Sklearn and fit on the embedding matrix created\n",
    "nn = NearestNeighbors(n_neighbors=6).fit(embedding_matrix) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the neighbours and place it in a matrix \n",
    "neighbours_mat = nn.kneighbors(embedding_matrix[1:vocab_size])[1]\n",
    "neighbours = {entry[0]: entry[1:] for entry in neighbours_mat}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "welli : ['naaa', 'hahahhahahahahah', 'yame', 'daaaang', 'jhn']\n",
      "nowt : ['noting', 'nothig', 'sth', 'reckon', 'tbf']\n",
      "apni : ['apne', 'kaam', 'karta', 'nahi', 'pehlay']\n",
      "hummed : ['exclaimed', 'groaned', 'audibly', 'nudged', 'nonchalantly']\n",
      "mole : ['pow', 'poo', 'schumachers', 'youpitch', 'wmgcamp']\n",
      "buried : ['trapped', 'bury', 'burying', 'laid', 'coffin']\n",
      "ohrwurm : ['caeczka', 'thedemocrats', 'tootime', 'suicidality', 'supportsmallstreamers']\n",
      "safely : ['landed', 'arrived', 'returned', 'heading', 'safe']\n",
      "colloidal : ['categorical', 'methamphetamine', 'hrk', 'rbg', 'strawman']\n",
      "hurt : ['hurts', 'hurting', 'feel', 'wont', 'reason']\n",
      "gallon : ['bucket', 'container', 'bottle', 'keg', 'pump']\n",
      "yee : ['hhaha', 'hah', 'coy', 'yah', 'yeh']\n",
      "stepped : ['slipped', 'jumped', 'stood', 'pushed', 'kicked']\n",
      "aquemini : ['qotsa', 'impetus', 'firstclass', 'onetime', 'tbqh']\n",
      "destructive : ['damaging', 'chaotic', 'partisan', 'dangerous', 'rhetoric']\n",
      "tolerant : ['intolerant', 'egocentric', 'economically', 'likable', 'nationalism']\n",
      "followed : ['replied', 'following', 'thx', 'noticed', 'thanks']\n",
      "thiz : ['ths', 'thag', 'thia', 'leik', 'wath']\n"
     ]
    }
   ],
   "source": [
    "# show how it works by prividing some examples\n",
    "for index in np.random.randint(1, vocab_size, 20):\n",
    "    if index in neighbours:\n",
    "        print(\"{} : {}\".format(str(index_word[index]), str([index_word[neighbours[index][i]] for i in range(5)])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_train_pos = text_train[label_train==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function that automatically loops around the sentence and changes word based on a given probability\n",
    "def change_sentence(sentence, neighbours):\n",
    "    for i in range(len(sentence)):\n",
    "        if np.random.random() > 0.5:\n",
    "            try:\n",
    "                syns = neighbours[sentence[i]]\n",
    "                sentence[i] = np.random.choice(syns)\n",
    "            except KeyError:\n",
    "                pass\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "indicies = np.random.randint(0, text_train_pos.shape[0], 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "just wanna sleep forever\n",
      "think wanna sleep forever\n",
      " \n",
      "never again never again youre killing me slow but i aint ready to die mtvhottest camila cabello\n",
      "never yet never right youre killing me low but but aint ready going als moots camila celeste\n",
      " \n",
      "im better off dead thats the damn truth right now\n",
      "im than up dead its the shit truth way again\n",
      " \n",
      "if i commit suicide at least a real nigga killed me\n",
      "know you committed attack from either another real bruh killed me\n",
      " \n",
      "my car fucked up at the worst time possible not only in the freeway but also a day before clean culture i throw the white flag someone please kill me now\n",
      "this truck fucks up at the worst break certain not if in the freeway but also a year after clean culture i throw the red parade someone pls hell me still\n",
      " \n",
      "im tired of living in florida this bland ass state\n",
      "im bored of living the florida this crass ass ohio\n",
      " \n",
      "my suicide note\n",
      "my suicide note\n",
      " \n",
      "cleaned my room still depressed will try again next week\n",
      "cleaned it floor really depressed will try again first weeks\n",
      " \n",
      "emon is taking forever im about to go to sleep on her ass\n",
      "jaillol is taking stay ill about for go going sleep on her dumb\n",
      " \n",
      "its am and i am the definition of self destruction and i kinda wish i were dead\n",
      "its am and i sure the definition of handle aftermath just but pretty hope i are dead\n",
      " \n"
     ]
    }
   ],
   "source": [
    "for sentence in text_train_pos[indicies]:\n",
    "    sample =  np.trim_zeros(sentence)\n",
    "    original_sentence = ' '.join([index_word[index] for index in sample])\n",
    "    print(original_sentence)\n",
    "\n",
    "    modified = change_sentence(sample, neighbours)\n",
    "    sentence_modified = ' '.join([index_word[index] for index in modified])\n",
    "    print(sentence_modified)\n",
    "    \n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_texts = 5000\n",
    "indexes = np.random.randint(0, text_train_pos.shape[0], n_texts)\n",
    "X_gen = np.array([change_sentence(x, neighbours) for x in text_train_pos[indexes]])\n",
    "y_gen = np.ones(n_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_train = np.concatenate((X_gen, text_train),axis=0)\n",
    "label_train = np.concatenate((y_gen, label_train),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15384, 50)\n",
      "(15384,)\n"
     ]
    }
   ],
   "source": [
    "print(text_train.shape)\n",
    "print(label_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(text_train)\n",
    "df['label'] = label_train\n",
    "df.to_csv('overclassed_data_trained.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emoji Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beyond the pre-trained glove weights on traditional English words, Emojis, recognized by some as bearing significant meanings, have been shown to aid in sentiment analysis on Twitter data and was therefore considered by some model designs. For every emoji used in the Twitter dataset, a random vector of length 100 is populated on the glove vector space. By setting the embedding layers to be trainable, the emoji vectors weights, in theory, could be learned in a way that their relationship to other words can be discerned, although experimentations demonstrated a less than significant difference in these modelsâ€™ predictive ability and their pretrained vector space counterparts and **are ultimately not used.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if the word is emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_emoji(s):\n",
    "    return s in emoji.UNICODE_EMOJI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from unicodedata import normalize\n",
    "\n",
    "\n",
    "# clean a list of lines\n",
    "def clean_txt(tweet):\n",
    "# prepare regex for char filtering\n",
    "    re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    re_print = re.compile('[^%s]' % re.escape(string.printable))\n",
    "# tokenize on white space\n",
    "    tweet = emoji.emojize(emoji.demojize(tweet).replace('::',': :'))\n",
    "#     tweet = normalize('NFD', tweet).encode('ascii', 'ignore')\n",
    "#     tweet = tweet.decode('UTF-8')\n",
    "    tweet = tweet.split()\n",
    "    tweet = [str(word) for word in tweet]\n",
    "    #remove usernames\n",
    "    tweet = [word for word in tweet if word[0] != '@']\n",
    "    # convert to lowercase\n",
    "    tweet = [word.lower() for word in tweet]\n",
    "    tweet = [word for word in tweet if word[0:4] != 'http']\n",
    "# remove punctuation from each token\n",
    "    tweet = [re_punc.sub('', w) for w in tweet]\n",
    "# remove non-printable chars form each token\n",
    "    tweet = [re_print.sub('', w) if not is_emoji(w) else w for w in tweet]\n",
    "# remove tokens with numbers in them\n",
    "    tweet = ['' if not word.isalpha() and not is_emoji(word) else word for word in tweet ]\n",
    "# store as string\n",
    "    tweet = (' '.join(tweet))\n",
    "    return ' '.join(tweet.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pain is bad makes me die ðŸ˜­ ðŸ˜© ðŸ¥º i eagerly await the next update'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_txt('@HoForBangtan Pain is bad makes me wann3a die ðŸ˜­ðŸ˜©ðŸ¥º\\nI eagerly await the next update')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove instances of 'RT' from beginning of tweet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "#remove RT:\n",
    "def rm_rt(tweet):\n",
    "    rt = re.compile(r'^(RT|rt)')\n",
    "    if rt.search(tweet):\n",
    "        return ''\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classified = pd.read_csv('cleaned_data.csv')[['label','text']]\n",
    "classified.text = classified.text.astype(str)\n",
    "classified.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use GloVe pretrained embeddings to seed embedding layer weights. Load GloVe weights trained on twitter dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1193514 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_idx = dict()\n",
    "glove_file = open('glove.twitter.27B.100d.txt')\n",
    "for line in glove_file:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_idx[word] = coefs\n",
    "glove_file.close()\n",
    "print('Loaded %s word vectors.' % len(embeddings_idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity, clump \"depressed\" and \"suicidal\" labels into one category (can refine this in future models/iteration; for first iteration, use binary classification):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1.])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = np.asarray(classified.text)\n",
    "labels_binary = classified.label.replace(to_replace=3, value=1)\n",
    "label = np.asarray(classified.label)\n",
    "\n",
    "np.unique(labels_binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize tweet texts. Use a max sentence length of 50 (with char limit of 280 and cleaning, this seems reasonable - by observation, most tweets still require padding), and pad all tweets out to this length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(text)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "# integer encode the documents\n",
    "encoded = tokenizer.texts_to_sequences(text)\n",
    "# pad documents to a max length of 4 words\n",
    "max_length = 50\n",
    "padded_tweets = pad_sequences(encoded, maxlen=max_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  9,   8, 800, ...,   0,   0,   0],\n",
       "       [ 34,  88,  90, ...,   0,   0,   0],\n",
       "       [ 14, 100,  53, ...,   0,   0,   0],\n",
       "       ...,\n",
       "       [ 31,  18, 777, ...,   0,   0,   0],\n",
       "       [686, 195,  12, ...,   0,   0,   0],\n",
       "       [378,  10, 986, ...,   0,   0,   0]], dtype=int32)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split into train/val set of 90%/10%;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_train, text_test, label_train, label_test = train_test_split(padded_tweets, labels_binary, test_size = 0.1, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize embedding matrix to seed embedding weights from the loaded GloVe embeddings:\n",
    "\n",
    "If emojis are considered in training the embedding layer, then a randomised weight list is initiated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((vocab_size, 100))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_idx.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    elif is_emoji(word):\n",
    "        np.random.seed(np.sum([ord(char) for char in word]))\n",
    "        embedding_matrix[i] = np.random.uniform(1,-1,100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting embedding matrix can be used in the embedding layer with `trainable = True`, but again, this did not yield any result of value and is therefore discarded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
